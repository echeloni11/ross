+ conda activate ross

CondaError: Run 'conda init' before 'conda activate'

++ hostname
+ MASTER_ADDR=j003-ds
+ MASTER_PORT=29805
+ torchrun --nproc-per-node=4 --nnodes 1 --node_rank 0 --master_addr=j003-ds --master_port=29805 train.py --per_device_train_batch_size 4 --gradient_accumulation_steps 8 --learning_rate 2e-5 --warmup_ratio 0.03 --deepspeed ./scripts/zero3.json --model_name_or_path HaochenWang/ross-qwen2-7b --pretrain_mm_mlp_adapter ./checkpoints/ross-siglip-qwen2-7b-pt558k/mm_projector.bin --pretrain_mm_inv_mlp_adapter ./checkpoints/ross-siglip-qwen2-7b-pt558k/mm_inv_projector.bin --output_dir ./checkpoints/rossqwen2-nuscenes20k-ross --vision_tower google/siglip-so400m-patch14-384 --version qwen_2 --mm_pixel_decoder ./pretrained_vae --data_path ./train_scenes.jsonl --image_folder '' --mm_projector_type mlp2x_gelu --mm_inv_projector_type denoiser_vit3x --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --num_train_epochs 3 --per_device_eval_batch_size 4 --evaluation_strategy no --save_strategy steps --save_on_each_epoch=True --save_steps 50 --save_total_limit 1 --save_only_model --weight_decay 0. --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 32768 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name rossqwen2-nuscenes20k-ross
[2025-03-01 03:42:09,251] torch.distributed.run: [WARNING] 
[2025-03-01 03:42:09,251] torch.distributed.run: [WARNING] *****************************************
[2025-03-01 03:42:09,251] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-03-01 03:42:09,251] torch.distributed.run: [WARNING] *****************************************
/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
Traceback (most recent call last):
  File "/home/hanyim/projects/ross/train.py", line 1288, in <module>
    train()
  File "/home/hanyim/projects/ross/train.py", line 1062, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/hf_argparser.py", line 348, in parse_args_into_dataclasses
    raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {remaining_args}")
ValueError: Some specified arguments are not used by the HfArgumentParser: ['--save_on_each_epoch=True']
Traceback (most recent call last):
  File "/home/hanyim/projects/ross/train.py", line 1288, in <module>
    train()
  File "/home/hanyim/projects/ross/train.py", line 1062, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/hf_argparser.py", line 348, in parse_args_into_dataclasses
    raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {remaining_args}")
ValueError: Some specified arguments are not used by the HfArgumentParser: ['--save_on_each_epoch=True']
Traceback (most recent call last):
  File "/home/hanyim/projects/ross/train.py", line 1288, in <module>
    train()
  File "/home/hanyim/projects/ross/train.py", line 1062, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/hf_argparser.py", line 348, in parse_args_into_dataclasses
    raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {remaining_args}")
ValueError: Some specified arguments are not used by the HfArgumentParser: ['--save_on_each_epoch=True']
Traceback (most recent call last):
  File "/home/hanyim/projects/ross/train.py", line 1288, in <module>
    train()
  File "/home/hanyim/projects/ross/train.py", line 1062, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/hf_argparser.py", line 348, in parse_args_into_dataclasses
    raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {remaining_args}")
ValueError: Some specified arguments are not used by the HfArgumentParser: ['--save_on_each_epoch=True']
[2025-03-01 03:43:24,345] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1501455) of binary: /net/scratch2/hanyim/envs/ross/bin/python
Traceback (most recent call last):
  File "/net/scratch2/hanyim/envs/ross/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-03-01_03:43:24
  host      : j003-ds
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1501456)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-03-01_03:43:24
  host      : j003-ds
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1501457)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2025-03-01_03:43:24
  host      : j003-ds
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 1501458)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-01_03:43:24
  host      : j003-ds
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1501455)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
