++ hostname
+ MASTER_ADDR=m001
+ MASTER_PORT=29805
+ torchrun --nproc-per-node=4 --nnodes 1 --node_rank 0 --master_addr=m001 --master_port=29805 train.py --per_device_train_batch_size 4 --gradient_accumulation_steps 8 --learning_rate 2e-5 --warmup_ratio 0.03 --deepspeed ./scripts/zero3.json --model_name_or_path Qwen/Qwen2-7B-Instruct --pretrain_mm_mlp_adapter ./checkpoints/ross-siglip-qwen2-7b-pt558k/mm_projector.bin --pretrain_mm_inv_mlp_adapter ./checkpoints/ross-siglip-qwen2-7b-pt558k/mm_inv_projector.bin --output_dir ./checkpoints/qwen2-nuscenes20k-ross --vision_tower google/siglip-so400m-patch14-384 --version qwen_2 --mm_pixel_decoder ./pretrained_vae --data_path ./train_scenes.jsonl --image_folder '' --mm_projector_type mlp2x_gelu --mm_inv_projector_type denoiser_vit3x --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --num_train_epochs 3 --per_device_eval_batch_size 4 --evaluation_strategy no --save_strategy epoch --save_steps 50 --save_total_limit 3 --save_only_model --weight_decay 0. --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 32768 --gradient_checkpointing True --gradient_checkpointing_kwargs '{"use_reentrant": false}' --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name qwen2-nuscenes20k-ross
[2025-03-01 22:18:21,257] torch.distributed.run: [WARNING] 
[2025-03-01 22:18:21,257] torch.distributed.run: [WARNING] *****************************************
[2025-03-01 22:18:21,257] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2025-03-01 22:18:21,257] torch.distributed.run: [WARNING] *****************************************
/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
You are using a model of type qwen2 to instantiate a model of type ross_qwen2. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Traceback (most recent call last):
  File "/home/hanyim/projects/ross/train.py", line 1336, in <module>
    train()
  File "/home/hanyim/projects/ross/train.py", line 1064, in train
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/hf_argparser.py", line 339, in parse_args_into_dataclasses
    obj = dtype(**inputs)
  File "<string>", line 148, in __init__
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/training_args.py", line 1730, in __post_init__
    self.device
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/training_args.py", line 2227, in device
    return self._setup_devices
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/utils/generic.py", line 60, in __get__
    cached = self.fget(obj)
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/transformers/training_args.py", line 2160, in _setup_devices
    self.distributed_state = PartialState(**accelerator_state_kwargs)
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/accelerate/state.py", line 275, in __init__
    self.set_device()
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/accelerate/state.py", line 786, in set_device
    device_module.set_device(self.device)
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/torch/cuda/__init__.py", line 404, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: uncorrectable ECC error encountered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

You are using a model of type qwen2 to instantiate a model of type ross_qwen2. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-03-01 22:18:46,309] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1805957 closing signal SIGTERM
[2025-03-01 22:18:46,309] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1805958 closing signal SIGTERM
[2025-03-01 22:18:46,309] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1805960 closing signal SIGTERM
[2025-03-01 22:18:47,177] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 2 (pid: 1805959) of binary: /net/scratch2/hanyim/envs/ross/bin/python
Traceback (most recent call last):
  File "/net/scratch2/hanyim/envs/ross/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/net/scratch2/hanyim/envs/ross/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-03-01_22:18:46
  host      : m001
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1805959)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
